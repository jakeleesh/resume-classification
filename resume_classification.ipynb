{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0b2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Resumes (For Applicants).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7323e",
   "metadata": {},
   "source": [
    "Some rows have these two columns swapped.\n",
    "Looking at the Excel file, rows that have been swapped are those where **Reason_for_decision** starts with \"expected_experience\".\n",
    "As such, we check for rows that start like that, and swap them with the **Job_Description** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbdaf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "swapped_indices = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    reason_col = str(row['Reason_for_decision']).strip().lower()\n",
    "    if reason_col.startswith('expected_experience'):\n",
    "        swapped_indices.append(idx)\n",
    "\n",
    "if len(swapped_indices) > 0:\n",
    "    for idx in swapped_indices:\n",
    "        df.loc[idx, 'Reason_for_decision'], df.loc[idx, 'Job_Description'] = df.loc[idx, 'Job_Description'], df.loc[idx, 'Reason_for_decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d71bdd8",
   "metadata": {},
   "source": [
    "Convert all data to lowercase as part of preprocessing of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58627e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = df.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33614667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email(text):\n",
    "    \"\"\"Gets email in Resume column using regex.\"\"\"\n",
    "    email = re.findall(r'\\S+@\\S+', str(text))\n",
    "    return email[0] if email else \"Not Found\"\n",
    "\n",
    "def get_phone(text):\n",
    "    \"\"\"Gets phone number following American format: 123-456-7890 or (123) 456-7890\"\"\"\n",
    "    phone = re.findall(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4})', str(text))\n",
    "    return phone[0] if phone else \"Not Found\"\n",
    "\n",
    "def get_name(text):\n",
    "    \"\"\"Gets name from resume column.\n",
    "\n",
    "    Names appear in specific patterns at the start of resumes:\n",
    "        1. **candidate profile: Name** or **data scientist candidate profile: Name**\n",
    "        2. Here's a sample/professional resume for Name\n",
    "        3. **Name** (just name in bold)\n",
    "        4. Plain name on first line\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = str(text).split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "        if not line_stripped:\n",
    "            continue\n",
    "            \n",
    "        # Pattern 1: **candidate profile: Name** or **data scientist candidate profile: Name**\n",
    "        match = re.search(r'\\*\\*.*?candidate profile:\\s*(.+?)\\*\\*', line_stripped, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        \n",
    "        # Pattern 2: Here's a sample/professional resume for Name\n",
    "        match = re.search(r\"here'?s?\\s+a\\s+(sample|professional)\\s+resume\\s+for\\s+(.+)\", line_stripped, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(2).strip()\n",
    "        \n",
    "        # Pattern 3: **Name** (just name in bold, no other text)\n",
    "        match = re.search(r'^\\*\\*(.+?)\\*\\*$', line_stripped)\n",
    "        if match:\n",
    "            name = match.group(1).strip()\n",
    "            # Make sure it's not a profile pattern\n",
    "            if 'candidate profile' not in name.lower():\n",
    "                return name\n",
    "        \n",
    "        # Pattern 4: Plain name (first non-empty line that doesn't match above)\n",
    "        # Limit line to less than 50 characters because names are not long.\n",
    "        if len(line_stripped) < 50 and not line_stripped.endswith('.'):\n",
    "            return line_stripped\n",
    "    \n",
    "    # If no text found, return \"Not Found\"\n",
    "    return \"Not Found\"\n",
    "\n",
    "def get_education(text):\n",
    "    \"\"\"\n",
    "    Extract education information based on the structured format:\n",
    "    - Looks for \"education:\" or \"**education:**\" line (lowercase since preprocessing lowercases all text)\n",
    "    - Gets the next non-empty line which contains the degree\n",
    "    - Format: * bachelor of science in computer science, xyz university (2010-2014)\n",
    "    - Returns only the degree part (before the comma), without university name\n",
    "    Note: All text is already lowercased during preprocessing\n",
    "    \"\"\"\n",
    "    lines = str(text).split('\\n')\n",
    "    found_education_header = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_stripped = line.strip()\n",
    "        \n",
    "        # Look for Education header (education: or **education:**) - all lowercase since preprocessing\n",
    "        if not found_education_header:\n",
    "            # Check if this line is the Education header (case-insensitive for safety)\n",
    "            if re.match(r'^\\*{0,2}education:\\*{0,2}$', line_stripped, re.IGNORECASE):\n",
    "                found_education_header = True\n",
    "                continue\n",
    "        else:\n",
    "            # We found the Education header, now look for the next non-empty line\n",
    "            if line_stripped:\n",
    "                # Clean the line: remove leading asterisks and bold markers\n",
    "                cleaned = re.sub(r'^\\*+\\s*', '', line_stripped)  # Remove leading asterisks\n",
    "                cleaned = re.sub(r'^\\*\\*(.+?)\\*\\*', r'\\1', cleaned)  # Remove bold markers\n",
    "                \n",
    "                # Extract degree (everything before the comma, which separates from university)\n",
    "                # Format: bachelor of science in computer science, xyz university (2010-2014)\n",
    "                # We want: bachelor of science in computer science\n",
    "                if ',' in cleaned:\n",
    "                    degree = cleaned.split(',')[0].strip()\n",
    "                    return degree\n",
    "                else:\n",
    "                    # If no comma, return the whole line (some might not have university)\n",
    "                    return cleaned\n",
    "    \n",
    "    # If no education found, return \"Not Found\"\n",
    "    return \"Not Found\"\n",
    "\n",
    "# This function extracts work experience (in years if mentioned)\n",
    "def get_experience(text):\n",
    "    # Pattern to find \"X years\" or \"X+ years\" or \"X-Y years\"\n",
    "    # Example: \"5 years of experience\" or \"3+ years in Python\"\n",
    "    exp = re.findall(r'(\\d+)\\+?\\s*(?:-\\s*\\d+)?\\s*years?', str(text).lower())\n",
    "    if exp:\n",
    "        # Convert the found number to integer and return it\n",
    "        # If multiple years mentioned, take the first one\n",
    "        return int(exp[0])\n",
    "    else:\n",
    "        # If no years mentioned, return 0\n",
    "        return 0\n",
    "\n",
    "# This function finds skills mentioned in the resume\n",
    "def get_skills(text):\n",
    "    # Define a comprehensive list of technical skills to look for\n",
    "    # These are common skills in tech jobs\n",
    "    keywords = [\n",
    "        'python', 'java', 'javascript', 'sql', 'r', 'c++', 'c#',\n",
    "        'machine learning', 'deep learning', 'data analysis', 'statistics',\n",
    "        'tableau', 'power bi', 'excel', 'aws', 'azure', 'cloud',\n",
    "        'git', 'docker', 'kubernetes', 'tensorflow', 'pytorch',\n",
    "        'html', 'css', 'react', 'angular', 'node.js', 'flask', 'django'\n",
    "    ]\n",
    "    # Convert resume text to lowercase for easier matching\n",
    "    text_lower = str(text).lower()\n",
    "    # Check each skill keyword and keep the ones found in the resume\n",
    "    # Use regex word boundaries (\\b) to match whole words only, preventing 'r' from matching 'ready'\n",
    "    found = [word for word in keywords if re.search(r'\\b' + re.escape(word) + r'\\b', text_lower)]\n",
    "    # Join all found skills with commas, or return \"None\" if no skills found\n",
    "    return \", \".join(found) if found else \"None\"\n",
    "\n",
    "# %% [4] Apply Extraction to All Resumes\n",
    "# Now we apply each extraction function to every resume in our dataset\n",
    "# This creates new columns in our table (dataframe) with the extracted info\n",
    "\n",
    "# Extract email from each resume and create a new 'Email' column\n",
    "df['Email'] = df['Resume'].apply(get_email)\n",
    "\n",
    "# Extract phone number from each resume and create a new 'Phone' column\n",
    "df['Phone'] = df['Resume'].apply(get_phone)\n",
    "\n",
    "# Extract name from each resume and create a new 'Name' column\n",
    "df['Name'] = df['Resume'].apply(get_name)\n",
    "\n",
    "# Extract education from each resume and create a new 'Education' column\n",
    "df['Education'] = df['Resume'].apply(get_education)\n",
    "\n",
    "# Extract years of experience from each resume\n",
    "df['Experience_Years'] = df['Resume'].apply(get_experience)\n",
    "\n",
    "# Extract skills from each resume and create a new 'Skills' column\n",
    "df['Skills'] = df['Resume'].apply(get_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c4ebc",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e87b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which jobs are most frequently applied for?\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Count how many applicants applied for each role and create a bar chart\n",
    "# value_counts() counts occurrences of each unique role\n",
    "# plot(kind='bar') makes a bar chart\n",
    "df['Role'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add a title to explain what this chart shows\n",
    "plt.title(\"Number of Applicants per Role\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Label the x-axis (horizontal) and y-axis (vertical)\n",
    "plt.xlabel(\"Job Role\", fontsize=12)\n",
    "plt.ylabel(\"Number of Applicants\", fontsize=12)\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees so they don't overlap\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add a grid in the background to make values easier to read\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# tight_layout() ensures nothing gets cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a47caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_education(text):\n",
    "    \"\"\"\n",
    "    Cleans raw education text and extracts the main field of study.\n",
    "    Returns standardized category names for easier analysis.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == \"Not Found\" or not text:\n",
    "        return \"Not Found\"\n",
    "    \n",
    "    # Convert to lowercase for easier pattern matching\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Define patterns for common fields of study\n",
    "    # Check in order of specificity (most specific first)\n",
    "    \n",
    "    if any(word in text_lower for word in ['computer science', 'cs', 'computer engineering', 'computing']):\n",
    "        return \"Computer Science\"\n",
    "    elif any(word in text_lower for word in ['data science', 'data analytics', 'data engineering']):\n",
    "        return \"Data Science\"\n",
    "    elif any(word in text_lower for word in ['information technology', 'it', 'information systems']):\n",
    "        return \"Information Technology\"\n",
    "    elif any(word in text_lower for word in ['software engineering', 'software development']):\n",
    "        return \"Software Engineering\"\n",
    "    elif any(word in text_lower for word in ['electrical engineering', 'electronics', 'ece']):\n",
    "        return \"Electrical Engineering\"\n",
    "    elif any(word in text_lower for word in ['mechanical engineering', 'manufacturing']):\n",
    "        return \"Mechanical Engineering\"\n",
    "    elif any(word in text_lower for word in ['civil engineering']):\n",
    "        return \"Civil Engineering\"\n",
    "    elif 'engineering' in text_lower:\n",
    "        return \"Engineering (Other)\"\n",
    "    elif any(word in text_lower for word in ['business administration', 'mba', 'business management']):\n",
    "        return \"Business Administration\"\n",
    "    elif any(word in text_lower for word in ['finance', 'accounting', 'economics']):\n",
    "        return \"Finance/Economics\"\n",
    "    elif any(word in text_lower for word in ['marketing', 'sales']):\n",
    "        return \"Marketing\"\n",
    "    elif any(word in text_lower for word in ['mathematics', 'statistics', 'math']):\n",
    "        return \"Mathematics/Statistics\"\n",
    "    elif any(word in text_lower for word in ['physics', 'chemistry', 'biology']):\n",
    "        return \"Natural Sciences\"\n",
    "    elif any(word in text_lower for word in ['arts', 'humanities', 'literature', 'history']):\n",
    "        return \"Arts/Humanities\"\n",
    "    elif any(word in text_lower for word in ['bachelor', 'master', 'phd', 'degree', 'diploma']):\n",
    "        return \"Other Degree\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "print(\"âœ“ Education cleaning function defined\")\n",
    "print(\"This function will categorize education backgrounds into standard fields\")\n",
    "\n",
    "df['Education_Clean'] = df['Education'].apply(clean_education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Count the top 15 most common education entries (using cleaned data)\n",
    "# We use .head(15) to get only the top 15 to avoid cluttering\n",
    "# Filter out \"Not Found\" entries for cleaner visualization\n",
    "education_counts = df[df['Education_Clean'] != 'Not Found']['Education_Clean'].value_counts().head(15)\n",
    "\n",
    "# Create a horizontal bar chart (barh means horizontal bars)\n",
    "# Horizontal is better when we have long text labels\n",
    "education_counts.plot(kind='barh', color='lightgreen', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Top 15 Most Common Education Backgrounds (By Degree Field)\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Number of Applicants\", fontsize=12)\n",
    "plt.ylabel(\"Education Background\", fontsize=12)\n",
    "\n",
    "# Add grid for easier reading\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Experience Levels\n",
    "# This shows the typical profile of candidates based on their experience\n",
    "\n",
    "# Create a figure with better size\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Create a histogram showing distribution of experience years\n",
    "# bins=20 means we divide the data into 20 groups\n",
    "# A histogram shows how many people fall into each experience range\n",
    "plt.hist(df['Experience_Years'], bins=20, color='coral', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Candidate Experience Levels\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Years of Experience\", fontsize=12)\n",
    "plt.ylabel(\"Number of Applicants\", fontsize=12)\n",
    "\n",
    "# Add a grid for easier reading\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skills Distribution Analysis\n",
    "# Count how many skills each person has (we'll use this throughout the analysis)\n",
    "df['Skill_Count'] = df['Skills'].apply(lambda x: len(str(x).split(',')) if x != 'None' else 0)\n",
    "\n",
    "# Create a histogram showing distribution of skill counts\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df['Skill_Count'], bins=15, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Technical Skill Diversity Among Candidates\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Number of Skills\", fontsize=12)\n",
    "plt.ylabel(\"Number of Applicants\", fontsize=12)\n",
    "\n",
    "# Add a grid for easier reading\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Patterns by Experience Level\n",
    "# Create experience level categories for better analysis\n",
    "df['Experience_Level'] = pd.cut(df['Experience_Years'], \n",
    "                                bins=[0, 2, 5, 10, 50],\n",
    "                                labels=['Entry (0-2 yrs)', 'Mid (3-5 yrs)', \n",
    "                                       'Senior (6-10 yrs)', 'Expert (10+ yrs)'])\n",
    "\n",
    "# Create a grouped analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Decision by Experience Level\n",
    "exp_decision = pd.crosstab(df['Experience_Level'], df['Decision'], normalize='index') * 100\n",
    "exp_decision.plot(kind='bar', ax=axes[0], color=['lightcoral', 'lightgreen'], edgecolor='black')\n",
    "axes[0].set_title(\"Selection Rate by Experience Level\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Experience Level\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[0].legend(title=\"Decision\", labels=['Reject', 'Select'])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%.1f%%', fontsize=9)\n",
    "\n",
    "# Right: Decision by Skill Count bins\n",
    "df['Skill_Level'] = pd.cut(df['Skill_Count'],\n",
    "                           bins=[0, 3, 6, 9, 30],\n",
    "                           labels=['Few (1-3)', 'Moderate (4-6)', 'Many (7-9)', 'Expert (10+)'])\n",
    "\n",
    "skill_decision = pd.crosstab(df['Skill_Level'], df['Decision'], normalize='index') * 100\n",
    "skill_decision.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightgreen'], edgecolor='black')\n",
    "axes[1].set_title(\"Selection Rate by Skill Count\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Skill Level\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Percentage (%)\", fontsize=12)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend(title=\"Decision\", labels=['Reject', 'Select'])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%.1f%%', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce93505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns in Hired vs Rejected Candidates\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "# subplot(1, 2, 1) means: 1 row, 2 columns, this is position 1\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left chart: Decision distribution (how many selected vs rejected)\n",
    "# value_counts() counts how many \"Select\" and how many \"Reject\"\n",
    "df['Decision'].value_counts().plot(kind='pie', ax=axes[0], autopct='%1.1f%%', \n",
    "                                    colors=['lightgreen', 'lightcoral'],\n",
    "                                    startangle=90)\n",
    "# autopct='%1.1f%%' means show percentages with 1 decimal place\n",
    "# startangle=90 means start the pie from the top\n",
    "\n",
    "# Add title to the left chart\n",
    "axes[0].set_title(\"Overall Hiring Decision Distribution\", fontsize=14, fontweight='bold')\n",
    "# Remove the y-label (pie charts don't need them)\n",
    "axes[0].set_ylabel('')\n",
    "\n",
    "# Right chart: Decision by Role\n",
    "# crosstab creates a table showing how many selected/rejected for each role\n",
    "decision_by_role = pd.crosstab(df['Role'], df['Decision'])\n",
    "# Plot as a stacked bar chart\n",
    "decision_by_role.plot(kind='bar', ax=axes[1], color=['lightcoral', 'lightgreen'])\n",
    "\n",
    "# Add title and labels to the right chart\n",
    "axes[1].set_title(\"Hiring Decisions by Role\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Job Role\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Number of Applicants\", fontsize=12)\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].legend(title=\"Decision\")\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Adjust layout so nothing overlaps\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skill Diversity vs. Experience (Scatter Plot with Regression Line)\n",
    "# Do people with more experience have more diverse skills?\n",
    "# If the line slopes upward, more experienced candidates are \"upskilling\"\n",
    "# If it's flat, skillsets might plateau after a certain career stage\n",
    "\n",
    "# Note: Skill_Count was already calculated in Cell 7b, so we can use it directly\n",
    "\n",
    "# Create a scatter plot (dots) showing experience vs number of skills\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# Each dot represents one candidate\n",
    "# x-axis is their experience, y-axis is their number of skills\n",
    "# alpha=0.5 makes dots semi-transparent so we can see overlapping points\n",
    "plt.scatter(df['Experience_Years'], df['Skill_Count'], alpha=0.5, \n",
    "            color='purple', edgecolors='black', s=60)\n",
    "\n",
    "# Add a regression line to see if there's a relationship\n",
    "# polyfit finds the best line through the data (degree 1 = straight line)\n",
    "z = np.polyfit(df['Experience_Years'], df['Skill_Count'], 1)\n",
    "p = np.poly1d(z)\n",
    "slope = z[0]  # Extract the slope of the line\n",
    "\n",
    "# Plot the regression line in red with dashes\n",
    "plt.plot(df['Experience_Years'], p(df['Experience_Years']), \n",
    "         \"r--\", linewidth=3, label=f'Regression Line (slope={slope:.3f})')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Skill Diversity vs. Experience: Are Candidates Upskilling?\", \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Years of Experience\", fontsize=13)\n",
    "plt.ylabel(\"Number of Unique Skills Identified\", fontsize=13)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "if slope > 0.1:\n",
    "    plt.text(0.5, 0.95, 'Positive Trend: More experience -> More skills', \n",
    "             transform=plt.gca().transAxes, fontsize=11, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "elif slope < -0.1:\n",
    "    plt.text(0.5, 0.95, 'Negative Trend: More experience -> Fewer unique skills listed', \n",
    "             transform=plt.gca().transAxes, fontsize=11, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "else:\n",
    "    plt.text(0.5, 0.95, 'Flat Trend: Experience and skills not strongly correlated', \n",
    "             transform=plt.gca().transAxes, fontsize=11, \n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e701e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Skills in Demand\n",
    "# What are the most commonly mentioned skills across all resumes?\n",
    "\n",
    "# Create an empty list to collect all skills\n",
    "all_skills = []\n",
    "\n",
    "# Go through each resume's skills\n",
    "for skills in df['Skills']:\n",
    "    # If skills exist (not \"None\")\n",
    "    if skills != 'None':\n",
    "        # Split by comma to get individual skills\n",
    "        # strip() removes extra spaces\n",
    "        skill_list = [s.strip() for s in str(skills).split(',')]\n",
    "        # Add these skills to our master list\n",
    "        all_skills.extend(skill_list)\n",
    "\n",
    "# Count how often each skill appears\n",
    "# pd.Series converts our list to a pandas object so we can use value_counts()\n",
    "skill_counts = pd.Series(all_skills).value_counts().head(15)\n",
    "\n",
    "# Create a bar chart of top skills\n",
    "plt.figure(figsize=(12, 6))\n",
    "skill_counts.plot(kind='barh', color='gold', edgecolor='black')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Top 15 Most In-Demand Technical Skills Across All Resumes\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Number of Resumes Mentioning This Skill\", fontsize=12)\n",
    "plt.ylabel(\"Skill\", fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Adjust and display\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553883db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Heatmap: Skills vs. Selection\n",
    "# Which skills lead to higher selection rates?\n",
    "# This heatmap reveals \"must-have\" skills that correlate with getting hired\n",
    "\n",
    "# Get top 15 most common skills\n",
    "top_15_skills = pd.Series(all_skills).value_counts().head(15).index.tolist()\n",
    "\n",
    "# Create a dataframe to store selection rates for each skill\n",
    "skill_selection_data = []\n",
    "\n",
    "for skill in top_15_skills:\n",
    "    # Check if this skill appears in the Skills column\n",
    "    # Split each row's skills and check for exact match (case-insensitive)\n",
    "    def has_this_skill(skills_text):\n",
    "        if skills_text == 'None' or pd.isna(skills_text):\n",
    "            return False\n",
    "        # Split by comma and check each skill\n",
    "        skill_items = [s.strip().lower() for s in str(skills_text).split(',')]\n",
    "        return skill.lower() in skill_items\n",
    "    \n",
    "    has_skill = df['Skills'].apply(has_this_skill)\n",
    "    \n",
    "    # Among those with this skill, how many were selected?\n",
    "    total_with_skill = has_skill.sum()\n",
    "    \n",
    "    if total_with_skill > 0:\n",
    "        selected_with_skill = (has_skill & (df['Decision'] == 'select')).sum()\n",
    "        rejected_with_skill = (has_skill & (df['Decision'] == 'reject')).sum()\n",
    "        \n",
    "        selection_rate = (selected_with_skill / total_with_skill) * 100\n",
    "        rejection_rate = (rejected_with_skill / total_with_skill) * 100\n",
    "        \n",
    "        skill_selection_data.append({\n",
    "            'Skill': skill.title(),\n",
    "            'Select': selection_rate,\n",
    "            'Reject': rejection_rate,\n",
    "            'Total': total_with_skill,\n",
    "            'Selected_Count': selected_with_skill,\n",
    "            'Rejected_Count': rejected_with_skill\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "skill_df = pd.DataFrame(skill_selection_data)\n",
    "\n",
    "# Prepare data for heatmap (transpose so skills are on x-axis)\n",
    "heatmap_data = skill_df[['Skill', 'Select', 'Reject']].set_index('Skill').T\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Use RdYlGn colormap (Red-Yellow-Green) where green = high selection rate\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Percentage (%)'}, \n",
    "            linewidths=1, linecolor='white',\n",
    "            vmin=0, vmax=100)\n",
    "\n",
    "plt.title('Decision Heatmap: Skills vs. Selection Rate\\n(Green = High Selection | Red = Low Selection)', \n",
    "            fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Top 15 Skills', fontsize=13)\n",
    "plt.ylabel('Hiring Decision', fontsize=13)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze reasons by decision type\n",
    "selected = df[df['Decision'] == 'select']['Reason_for_decision']\n",
    "rejected = df[df['Decision'] == 'reject']['Reason_for_decision']\n",
    "\n",
    "# Define categories and their keywords to extract patterns\n",
    "reason_categories = {\n",
    "    'Experience Issues': [\n",
    "        'lack', 'lacks', 'insufficient', 'limited', 'no experience', \n",
    "        'expected_experience', 'not enough', 'minimal', 'inadequate experience'\n",
    "    ],\n",
    "    'Technical Skills': [\n",
    "        'technical skills', 'strong technical', 'proficient', 'expertise',\n",
    "        'proficiency', 'skilled in', 'advanced knowledge'\n",
    "    ],\n",
    "    'Cloud/DevOps': [\n",
    "        'cloud', 'aws', 'azure', 'kubernetes', 'docker', 'devops',\n",
    "        'cloud platforms', 'infrastructure'\n",
    "    ],\n",
    "    'Cultural Fit': [\n",
    "        'cultural fit', 'culture', 'team fit', 'alignment', 'values'\n",
    "    ],\n",
    "    'Communication/Soft Skills': [\n",
    "        'communication', 'soft skills', 'interpersonal', 'collaboration',\n",
    "        'teamwork', 'presentation'\n",
    "    ],\n",
    "    'Leadership/Management': [\n",
    "        'leadership', 'management', 'lead', 'mentor', 'senior', 'strategic'\n",
    "    ],\n",
    "    'System Design/Architecture': [\n",
    "        'system design', 'architecture', 'design patterns', 'scalability',\n",
    "        'architectural'\n",
    "    ],\n",
    "    'Domain Knowledge': [\n",
    "        'domain', 'industry', 'business knowledge', 'sector'\n",
    "    ],\n",
    "    'Full-Stack/Development': [\n",
    "        'full-stack', 'full stack', 'front-end', 'back-end', \n",
    "        'frontend', 'backend', 'development experience'\n",
    "    ],\n",
    "    'Data/Analytics': [\n",
    "        'data analysis', 'analytics', 'data science', 'machine learning',\n",
    "        'ai', 'ml', 'statistical'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Count occurrences for rejected candidates\n",
    "rejection_pattern_counts = {}\n",
    "for category, keywords in reason_categories.items():\n",
    "    count = 0\n",
    "    for keyword in keywords:\n",
    "        count += rejected.str.lower().str.contains(keyword, regex=False, na=False).sum()\n",
    "    if count > 0:\n",
    "        rejection_pattern_counts[category] = count\n",
    "\n",
    "# Count occurrences for selected candidates\n",
    "selection_pattern_counts = {}\n",
    "for category, keywords in reason_categories.items():\n",
    "    count = 0\n",
    "    for keyword in keywords:\n",
    "        count += selected.str.lower().str.contains(keyword, regex=False, na=False).sum()\n",
    "    if count > 0:\n",
    "        selection_pattern_counts[category] = count\n",
    "\n",
    "# Visualize patterns in hiring/rejection reasons\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Prepare data - get top categories from both\n",
    "all_categories = set(list(rejection_pattern_counts.keys()) + list(selection_pattern_counts.keys()))\n",
    "categories_list = sorted(all_categories)\n",
    "\n",
    "# Calculate percentages for comparison\n",
    "reject_pcts = []\n",
    "select_pcts = []\n",
    "for cat in categories_list:\n",
    "    reject_count = rejection_pattern_counts.get(cat, 0)\n",
    "    select_count = selection_pattern_counts.get(cat, 0)\n",
    "    reject_pcts.append((reject_count / len(rejected)) * 100)\n",
    "    select_pcts.append((select_count / len(selected)) * 100)\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(categories_list))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh(x - width/2, reject_pcts, width, label='Rejected', color='#E74C3C', alpha=0.8)\n",
    "bars2 = ax.barh(x + width/2, select_pcts, width, label='Selected', color='#2ECC71', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Percentage of Candidates', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Reason Category', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hiring Decision Patterns: What Matters for Selection vs Rejection\\n(Based on \"Reason_for_decision\" text analysis)', \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(categories_list)\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        width_val = bar.get_width()\n",
    "        if width_val > 1:  # Only show label if > 1%\n",
    "            ax.text(width_val + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{width_val:.1f}%', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cee0e",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c9f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - First, we convert extracted data (Education, Experience, Skills) into numbers\n",
    "# - Then, we build: Model 1 (Role Classifier) + Model 2 (Role-Specific Models)\n",
    "\n",
    "# 1. (Convert Structured Data to Numbers)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Experience Years\n",
    "# This is already a number (0-15 years), but we need to normalize it\n",
    "# Normalization puts all features on a similar scale so one doesn't dominate\n",
    "X_experience = df['Experience_Years'].values.reshape(-1, 1)  # Reshape to column vector\n",
    "# StandardScaler makes the average=0 and standard deviation=1\n",
    "# Example: 5 years might become 0.2, 10 years becomes 1.5, etc.\n",
    "experience_scaler = StandardScaler()\n",
    "X_experience = experience_scaler.fit_transform(X_experience)\n",
    "\n",
    "# Education Level\n",
    "# Education is text like \"bachelor of science\", \"master of arts\", \"phd\"\n",
    "# We convert this to numbers using LabelEncoder\n",
    "# LabelEncoder assigns each unique education level a number\n",
    "education_encoder = LabelEncoder()\n",
    "X_education = education_encoder.fit_transform(df['Education']).reshape(-1, 1)\n",
    "\n",
    "# Individual Skills (One feature per skill)\n",
    "# Skills is like \"python, sql, react\" - we want to create separate features\n",
    "# for each skill: has_python=1/0, has_sql=1/0, has_react=1/0\n",
    "# CountVectorizer can do this if we treat skills like a document\n",
    "# Create a custom tokenizer that splits on commas\n",
    "# This treats \"python, sql, react\" as three separate tokens\n",
    "def skill_tokenizer(text):\n",
    "    # Split by comma, strip whitespace, and lowercase\n",
    "    if text == 'None' or pd.isna(text):\n",
    "        return []\n",
    "    return [skill.strip().lower() for skill in str(text).split(',')]\n",
    "\n",
    "# CountVectorizer with binary=True creates 0/1 features for each skill\n",
    "# binary=True means we only care if skill EXISTS (1) or NOT (0), not how many times\n",
    "skills_vectorizer = CountVectorizer(\n",
    "    tokenizer=skill_tokenizer,\n",
    "    lowercase=False,  # Already lowercased in tokenizer\n",
    "    token_pattern=None,  # Explicitly set to None since we're using a custom tokenizer\n",
    "    binary=True       # Each skill is just 0 (absent) or 1 (present)\n",
    ")\n",
    "\n",
    "X_skills = skills_vectorizer.fit_transform(df['Skills'])\n",
    "skill_names = skills_vectorizer.get_feature_names_out()\n",
    "\n",
    "# COMBINE ALL FEATURES\n",
    "# Stack all features together horizontally (side by side)\n",
    "# Final result: [1 experience | 1 education | N skills] features\n",
    "# Convert numpy arrays to sparse matrices so they match X_skills format\n",
    "X_experience_sparse = csr_matrix(X_experience)\n",
    "X_education_sparse = csr_matrix(X_education)\n",
    "\n",
    "# Stack everything together: [Experience | Education | Skills]\n",
    "X = hstack([X_experience_sparse, X_education_sparse, X_skills])\n",
    "\n",
    "# 2. TRAIN ROLE CLASSIFIER (Which role fits best?)\n",
    "y_role = df['Role']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# Think of it like: 80% for teaching, 20% for the final exam\n",
    "# random_state=42 ensures we get the same split every time (reproducibility)\n",
    "# stratify ensures each role appears proportionally in both train and test sets\n",
    "X_train_role, X_test_role, y_train_role, y_test_role = train_test_split(\n",
    "    X,                          # All features (Education, Experience, Skills)\n",
    "    y_role,                     # All role labels\n",
    "    test_size=0.2,              # 20% for testing, 80% for training\n",
    "    random_state=42,            # Makes results reproducible\n",
    "    stratify=y_role             # Keep role proportions balanced\n",
    ")\n",
    "\n",
    "# Random Forest is like having many decision trees vote together\n",
    "# Each tree looks at different patterns, then they vote on the answer\n",
    "# Create the model with specific settings:\n",
    "role_classifier = RandomForestClassifier(\n",
    "    n_estimators=200,          # Use 200 trees (more trees = more accurate, but slower)\n",
    "    max_depth=30,              # Each tree can be 30 levels deep (prevents overcomplicating)\n",
    "    min_samples_split=5,       # Need at least 5 resumes to make a split decision\n",
    "    min_samples_leaf=2,        # Each final decision needs at least 2 resumes\n",
    "    random_state=42,           # Makes results reproducible\n",
    "    class_weight='balanced'    # Give equal importance to all roles (even if some have few examples)\n",
    ")\n",
    "\n",
    "# Train the model (this is where the learning happens!)\n",
    "# fit means: \"learn the patterns from training data\"\n",
    "role_classifier.fit(X_train_role, y_train_role)\n",
    "\n",
    "# Test the model on data it hasn't seen before\n",
    "# predict means: \"make your best guess for these resumes\"\n",
    "role_predictions = role_classifier.predict(X_test_role)\n",
    "\n",
    "# Calculate accuracy: How many did we get right?\n",
    "# accuracy_score compares predictions to actual answers\n",
    "role_accuracy = accuracy_score(y_test_role, role_predictions)\n",
    "\n",
    "# 3: TRAIN ROLE-SPECIFIC SUITABILITY MODELS\n",
    "# For each role, train a model that knows what 'qualified' means\n",
    "\n",
    "# Dictionary to store all role-specific models\n",
    "# Think of it as a filing cabinet where each role has its own trained model\n",
    "role_specific_models = {}\n",
    "# Dictionary to store accuracy for each model (for reporting)\n",
    "role_model_accuracies = {}\n",
    "# Get list of all unique roles in our data\n",
    "all_roles = sorted(df['Role'].unique())\n",
    "\n",
    "# Train one model for each role\n",
    "for role in all_roles:\n",
    "    role_mask = df['Role'] == role\n",
    "    role_df = df[role_mask]\n",
    "    \n",
    "    # Count how many select vs reject for this role\n",
    "    select_count = (role_df['Decision'] == 'select').sum()\n",
    "    reject_count = (role_df['Decision'] == 'reject').sum()\n",
    "    total_count = len(role_df)\n",
    "    \n",
    "    # Skip if too few examples (need at least 10 to train properly)\n",
    "    if total_count < 10:\n",
    "        continue\n",
    "    \n",
    "    # Get the features (X) and labels (y) for this role only\n",
    "    # IMPORTANT: Use .values to convert pandas Series to numpy array for sparse matrix indexing\n",
    "    X_role = X[role_mask.values]           # Features for this role (Education, Experience, Skills)\n",
    "    y_role_decision = df.loc[role_mask, 'Decision']  # Select/Reject decisions for this role\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    # Use 20% for testing, 80% for training\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_role,\n",
    "            y_role_decision,\n",
    "            test_size=0.2,          # 20% for testing\n",
    "            random_state=42,\n",
    "            stratify=y_role_decision  # Keep select/reject proportions balanced\n",
    "        )\n",
    "    except ValueError:\n",
    "        # If we can't split (e.g., only 1 class), skip this role\n",
    "        continue\n",
    "    \n",
    "    # Train the model for this specific role\n",
    "    # This model learns what makes someone qualified for THIS role\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,          # Use 100 trees (fewer than role classifier since less data)\n",
    "        max_depth=20,              # Maximum depth of 20 (simpler than role classifier)\n",
    "        min_samples_split=5,       # Need 5 samples to split\n",
    "        min_samples_leaf=2,        # Need 2 samples in final decision\n",
    "        random_state=42,\n",
    "        class_weight='balanced'    # Handle imbalanced data (more selects or rejects)\n",
    "    )\n",
    "    \n",
    "    # Train the model on this role's data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test the model\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Store the trained model and its accuracy\n",
    "    role_specific_models[role] = model\n",
    "    role_model_accuracies[role] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccf0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison - Testing Different Algorithms for Role Classification\n",
    "# Compare Random Forest with other ML algorithms for ROLE prediction\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "role_models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42, solver='saga'),\n",
    "    'Support Vector Machine': SVC(kernel='linear', random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "role_results = {}\n",
    "\n",
    "for name, clf in role_models.items():\n",
    "    clf.fit(X_train_role, y_train_role)\n",
    "    pred = clf.predict(X_test_role)\n",
    "    acc = accuracy_score(y_test_role, pred)\n",
    "    role_results[name] = acc\n",
    "\n",
    "# Calculate average accuracy of role-specific models\n",
    "avg_role_specific_accuracy = np.mean(list(role_model_accuracies.values()))\n",
    "\n",
    "# Visualize Model Comparisons\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Role Classifier Comparison\n",
    "model_names = list(role_results.keys())\n",
    "accuracies = list(role_results.values())\n",
    "\n",
    "ax.bar(model_names, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'],\n",
    "        edgecolor='black', alpha=0.8)\n",
    "ax.set_title('Role Classifier - Algorithm Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0%', '20%', '40%', '60%', '80%', '100%'])\n",
    "ax.axhline(y=0.8, color='red', linestyle='--', linewidth=2, alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (name, acc) in enumerate(role_results.items()):\n",
    "    ax.text(i, acc, f'{acc*100:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f44c6",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a278591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models for Production Deployment\n",
    "# We need to save: 1 role classifier + multiple role-specific models\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "# Role Classifier model predicts which job role best fits a resume\n",
    "joblib.dump(role_classifier, 'artifacts/role_classifier.pkl')\n",
    "\n",
    "# Role-specifi model knows how to evaluate candidates for its specific role\n",
    "joblib.dump(role_specific_models, 'artifacts/role_specific_models.pkl')\n",
    "\n",
    "# Transformers used to convert structured data to numbers\n",
    "joblib.dump(experience_scaler, 'artifacts/experience_scaler.pkl')\n",
    "joblib.dump(education_encoder, 'artifacts/education_encoder.pkl')\n",
    "joblib.dump(skills_vectorizer, 'artifacts/skills_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-classification (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
